{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/magnusharder/Documents/UNI-DTU/6. Semester/Bachelor Projekt/News-Recommendations/.venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from TestData.MindDependencies.MindIt import MINDIterator\n",
    "from TestData.MindDependencies.Utils import get_mind_data_set\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "\n",
    "from General.Utils import ValidateModel\n",
    "from DataIterator import NewsDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "import torch as th\n",
    "import numpy as np\n",
    "import yaml\n",
    "\n",
    "# Import Hparam\n",
    "with open('Data/MINDdemo_utils/lstur.yaml','r') as stream:\n",
    "    hparams = yaml.safe_load(stream)\n",
    "\n",
    "# Import word_vec\n",
    "word_embedding = np.load('Data/MINDdemo_utils/embedding_all.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Device\n",
    "device = 'cuda' if th.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Define Data, Dataset and DataLoaders\n",
    "train_behaviors_file = 'Data/MINDdemo_train/behaviors.tsv'\n",
    "train_news_file = 'Data/MINDdemo_train/news.tsv'\n",
    "word_dict_file = 'Data/MINDdemo_utils/word_dict_all.pkl'\n",
    "user_dict_file = 'Data/MINDdemo_utils/uid2index.pkl'\n",
    "\n",
    "valid_behaviors_file = 'Data/MINDdemo_dev/behaviors.tsv'\n",
    "valid_news_file = 'Data/MINDdemo_dev/news.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open (\"Data/MINDdemo_utils/word_dict.pkl\", \"rb\") as f:\n",
    "    word_dict = pickle.load(f)\n",
    "with open (\"Data/MINDdemo_utils/uid2index.pkl\", \"rb\") as f:\n",
    "    uid2index = pickle.load(f)\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class HyperParams:\n",
    "    batch_size: int\n",
    "    title_size: int\n",
    "    his_size: int\n",
    "    wordDict_file: str\n",
    "    userDict_file: str\n",
    "\n",
    "hparamsdata = HyperParams(\n",
    "    batch_size=32,\n",
    "    title_size=20,\n",
    "    his_size=50,\n",
    "    wordDict_file=word_dict_file,\n",
    "    userDict_file=user_dict_file,\n",
    ")\n",
    "\n",
    "train_iterator = MINDIterator(hparamsdata,npratio=4)\n",
    "test_iterator = MINDIterator(hparamsdata)\n",
    "\n",
    "batch_loader_train = train_iterator.load_data_from_file(train_news_file, train_behaviors_file)\n",
    "batch_loader_valid = test_iterator.load_data_from_file(valid_news_file, valid_behaviors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TestData.LSTURMind import LSTURini\n",
    "\n",
    "\n",
    "# Set Model Architecture\n",
    "LSTUR_con_module = LSTURini(\n",
    "    attention_dim = hparams['model']['attention_hidden_dim'],\n",
    "    word_emb_dim = hparams['model']['word_emb_dim'],\n",
    "    dropout = hparams['model']['dropout'],\n",
    "    filter_num = hparams['model']['filter_num'],\n",
    "    windows_size = hparams['model']['window_size'],\n",
    "    gru_unit = hparams['model']['gru_unit'],\n",
    "    user_size = train_iterator.uid2index.__len__() + 1,\n",
    "    word_vectors = word_embedding,\n",
    "    device = device\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "model = LSTUR_con_module.to(device)\n",
    "\n",
    "# Define Optimizer\n",
    "optimizer = th.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "loss_fn = th.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define Loss\n",
    "# def loss_fn(Scores,n_positive):\n",
    "#     n = Scores.shape[0]\n",
    "\n",
    "#     loss = 0\n",
    "#     for i in range(n):\n",
    "#         loss += -th.log(th.exp(Scores[i,:n_positive[i],0])/th.exp(Scores[i,:n_positive[i],:]).sum(dim=1)).sum()\n",
    "\n",
    "#     return loss/n\n",
    "\n",
    "def loss_fn_vali(Scores,labels):\n",
    "\n",
    "    loss = -th.log(th.exp(Scores[labels == 1].sum())/th.exp(Scores).sum())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_tensor(batch):\n",
    "    user_id = th.from_numpy(batch['user_index_batch'])\n",
    "    history_title = th.from_numpy(batch['clicked_title_batch'])\n",
    "    impressions_title = th.from_numpy(batch['candidate_title_batch'])\n",
    "    labels = th.from_numpy(batch['labels'])\n",
    "\n",
    "    return user_id, history_title, impressions_title, labels\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:20,  1.08s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "take(): Expected a long tensor for index, but got Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m     AUC_pre \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m ValidateModel\u001b[39m.\u001b[39mROC_AUC(preds[key], labels_dc[key])\n\u001b[1;32m     60\u001b[0m     loss_pre \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_fn_vali(th\u001b[39m.\u001b[39mtensor(preds[key]), th\u001b[39m.\u001b[39mtensor(labels_dc[key]))\n\u001b[0;32m---> 61\u001b[0m     MRR_pre \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m ValidateModel\u001b[39m.\u001b[39;49mmean_reciprocal_rank(th\u001b[39m.\u001b[39;49mtensor(labels_dc[key]), th\u001b[39m.\u001b[39;49mtensor(preds[key]))\n\u001b[1;32m     63\u001b[0m \u001b[39m# MRR_pre = MRR_pre/len(preds.keys())\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m# Calculate average metrics\u001b[39;00m\n\u001b[1;32m     66\u001b[0m AUC_pre \u001b[39m=\u001b[39m AUC_pre\u001b[39m/\u001b[39mi\n",
      "File \u001b[0;32m~/Documents/UNI-DTU/6. Semester/Bachelor Projekt/News-Recommendations/General/Utils.py:42\u001b[0m, in \u001b[0;36mValidateModel.mean_reciprocal_rank\u001b[0;34m(y_true, y_score)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmean_reciprocal_rank\u001b[39m(y_true, y_score):\n\u001b[1;32m     41\u001b[0m     order \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mtopk(y_score,k\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(y_score))\u001b[39m.\u001b[39mindices\n\u001b[0;32m---> 42\u001b[0m     rank \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39;49mtake(order,y_true) \u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     43\u001b[0m     rr_score \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m/\u001b[39m rank\n\u001b[1;32m     44\u001b[0m     \u001b[39mreturn\u001b[39;00m rr_score\n",
      "\u001b[0;31mRuntimeError\u001b[0m: take(): Expected a long tensor for index, but got Float"
     ]
    }
   ],
   "source": [
    "# Pre Training Validation step\n",
    "model.train(False)\n",
    "\n",
    "softmax = th.nn.Softmax(dim=1)\n",
    "preds = {i : [] for i in test_iterator.impr_indexes}\n",
    "labels_dc = {i : [] for i in test_iterator.impr_indexes}\n",
    "\n",
    "with th.no_grad():\n",
    "    \n",
    "    # Initialize variables\n",
    "    AUC_pre= 0\n",
    "    MRR_pre= 0\n",
    "    loss_pre = 0\n",
    "\n",
    "    # Load validation data\n",
    "    i = 0\n",
    "\n",
    "\n",
    "    # Loop through validation data)\n",
    "    for batch in tqdm(batch_loader_valid):\n",
    "        i += 1\n",
    "\n",
    "        # Load batch\n",
    "        user_id, history_title, impressions_title, labels = batch_to_tensor(batch)\n",
    "\n",
    "        Scores = model(user_id.flatten(), history_title, impressions_title)\n",
    "\n",
    "        for idx, id in enumerate(batch['impression_index_batch']):\n",
    "            preds[id.item()].append( Scores[idx].item())\n",
    "            labels_dc[id.item()].append(labels[idx].item())\n",
    "\n",
    "        pred = softmax(Scores)\n",
    "        #print(pred)\n",
    "        # Calculate loss\n",
    "        #loss = loss_fn_vali(Scores,labels)\n",
    "        #loss_pre += loss.item()\n",
    "\n",
    "\n",
    "        # # Calculate metrics\n",
    "        #AUC_score = ValidateModel.ROC_AUC(Scores.detach().cpu(), labels.detach().cpu())\n",
    "        #MRR_score = ValidateModel.mean_reciprocal_rank(Clicked.detach().cpu(), pred.detach().cpu()[0])\n",
    "\n",
    "        #AUC_pre += AUC_score\n",
    "        #MRR_pre += MRR_score.item()/N_vali\n",
    "        if i == 20:\n",
    "            break\n",
    "    \n",
    "    for key in preds.keys():\n",
    "        # preds[key] = np.array(preds[key])\n",
    "        # preds[key] = preds[key].argsort()[::-1]\n",
    "        # preds[key] = np.where(preds[key] == 0)[0][0]\n",
    "        # preds[key] = 1/(preds[key] + 1)\n",
    "        # MRR_pre += preds[key]\n",
    "        if len(preds[key]) == 0:\n",
    "            continue\n",
    "        if not 1 in labels_dc[key]:\n",
    "            continue\n",
    "\n",
    "        AUC_pre += ValidateModel.ROC_AUC(preds[key], labels_dc[key])\n",
    "        loss_pre += loss_fn_vali(th.tensor(preds[key]), th.tensor(labels_dc[key]))\n",
    "        #MRR_pre += ValidateModel.mean_reciprocal_rank(th.tensor(labels_dc[key]), th.tensor(preds[key]))\n",
    "\n",
    "    # MRR_pre = MRR_pre/len(preds.keys())\n",
    "\n",
    "    # Calculate average metrics\n",
    "    AUC_pre = AUC_pre/i\n",
    "    #MRR_pre = MRR_pre/i\n",
    "    loss_pre = loss_pre/i\n",
    "\n",
    "print(f\"Pre Training AUC: {AUC_pre}, MRR: {MRR_pre}, Loss: {loss_pre}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
