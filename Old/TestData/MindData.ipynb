{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from TestData.MindDependencies.MindIt import MINDIterator\n",
    "from TestData.MindDependencies.Utils import get_mind_data_set, validate_model\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "\n",
    "from General.Utils import ValidateModel\n",
    "from DataIterator import NewsDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "import torch as th\n",
    "import numpy as np\n",
    "import yaml\n",
    "\n",
    "# Import Hparam\n",
    "with open('Data/MINDdemo_utils/lstur.yaml','r') as stream:\n",
    "    hparams = yaml.safe_load(stream)\n",
    "\n",
    "# Import word_vec\n",
    "word_embedding = np.load('Data/MINDdemo_utils/embedding_all.npy')\n",
    "word_embedding = word_embedding.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Device\n",
    "device = 'cuda' if th.cuda.is_available() else 'mps'\n",
    "\n",
    "# Define Data, Dataset and DataLoaders\n",
    "train_behaviors_file = 'Data/MINDdemo_train/behaviors.tsv'\n",
    "train_news_file = 'Data/MINDdemo_train/news.tsv'\n",
    "word_dict_file = 'Data/MINDdemo_utils/word_dict_all.pkl'\n",
    "user_dict_file = 'Data/MINDdemo_utils/uid2index.pkl'\n",
    "\n",
    "valid_behaviors_file = 'Data/MINDdemo_dev/behaviors.tsv'\n",
    "valid_news_file = 'Data/MINDdemo_dev/news.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open (\"Data/MINDdemo_utils/word_dict.pkl\", \"rb\") as f:\n",
    "    word_dict = pickle.load(f)\n",
    "with open (\"Data/MINDdemo_utils/uid2index.pkl\", \"rb\") as f:\n",
    "    uid2index = pickle.load(f)\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class HyperParams:\n",
    "    batch_size: int\n",
    "    title_size: int\n",
    "    his_size: int\n",
    "    wordDict_file: str\n",
    "    userDict_file: str\n",
    "\n",
    "hparamsdata = HyperParams(\n",
    "    batch_size=32,\n",
    "    title_size=20,\n",
    "    his_size=50,\n",
    "    wordDict_file=word_dict_file,\n",
    "    userDict_file=user_dict_file,\n",
    ")\n",
    "\n",
    "train_iterator = MINDIterator(hparamsdata,npratio=4)\n",
    "test_iterator = MINDIterator(hparamsdata)\n",
    "\n",
    "batch_loader_train = train_iterator.load_data_from_file(train_news_file, train_behaviors_file)\n",
    "batch_loader_valid = test_iterator.load_data_from_file(valid_news_file, valid_behaviors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/magnusharder/Documents/UNI-DTU/6. Semester/Bachelor Projekt/News-Recommendations/TestData/LSTURMind.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.word_embedding = nn.Embedding.from_pretrained(th.tensor(word_vectors,dtype=th.float32), freeze=False, padding_idx=0)\n"
     ]
    }
   ],
   "source": [
    "from TestData.LSTURMind import LSTURini\n",
    "\n",
    "\n",
    "# Set Model Architecture\n",
    "LSTUR_con_module = LSTURini(\n",
    "    attention_dim = hparams['model']['attention_hidden_dim'],\n",
    "    word_emb_dim = hparams['model']['word_emb_dim'],\n",
    "    dropout = hparams['model']['dropout'],\n",
    "    filter_num = hparams['model']['filter_num'],\n",
    "    windows_size = hparams['model']['window_size'],\n",
    "    gru_unit = hparams['model']['gru_unit'],\n",
    "    user_size = train_iterator.uid2index.__len__() + 1,\n",
    "    word_vectors = th.from_numpy(word_embedding).to(device),\n",
    "    device = device\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "model = LSTUR_con_module.to(device)\n",
    "\n",
    "# Define Optimizer\n",
    "optimizer = th.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "loss_fn = th.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define Loss\n",
    "# def loss_fn(Scores,n_positive):\n",
    "#     n = Scores.shape[0]\n",
    "\n",
    "#     loss = 0\n",
    "#     for i in range(n):\n",
    "#         loss += -th.log(th.exp(Scores[i,:n_positive[i],0])/th.exp(Scores[i,:n_positive[i],:]).sum(dim=1)).sum()\n",
    "\n",
    "#     return loss/n\n",
    "\n",
    "def loss_fn_vali(Scores,labels):\n",
    "\n",
    "    loss = -th.log(th.exp(Scores[labels == 1].sum())/th.exp(Scores).sum())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "586it [00:06, 85.49it/s]\n",
      "236it [02:30,  1.57it/s]\n",
      "7538it [00:38, 193.99it/s]\n"
     ]
    }
   ],
   "source": [
    "Pre_training = validate_model(model, valid_news_file, valid_behaviors_file, test_iterator, device, metrics=['group_auc', 'mean_mrr', 'ndcg@5;10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_tensor(batch, device):\n",
    "    user_id = th.from_numpy(batch['user_index_batch']).to(device).flatten()\n",
    "    history_title = th.from_numpy(batch['clicked_title_batch']).to(device)\n",
    "    impressions_title = th.from_numpy(batch['candidate_title_batch']).to(device)\n",
    "    labels = th.from_numpy(batch['labels']).to(device)\n",
    "\n",
    "    return user_id, history_title, impressions_title, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "443it [19:29,  2.60s/it]"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "AUC = [Pre_training['group_auc']]\n",
    "MRR = [Pre_training['mean_mrr']]\n",
    "NDCG5 = [Pre_training['ndcg@5']]\n",
    "NDCG10 = [Pre_training['ndcg@10']]\n",
    "\n",
    "\n",
    "for epoch in range(1):\n",
    "\n",
    "    for batch in tqdm(train_iterator.load_data_from_file(train_news_file, train_behaviors_file)):\n",
    "\n",
    "        user_id, history_title, impressions_title, labels = batch_to_tensor(batch,device)\n",
    "\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        Scores = model(user_id, history_title, impressions_title)\n",
    "\n",
    "        loss = loss_fn(Scores,labels)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "    with th.no_grad():\n",
    "        model.eval()\n",
    "        model.train(False)\n",
    "\n",
    "        result = validate_model(model, valid_news_file, valid_behaviors_file, test_iterator, device, metrics=['group_auc', 'mean_mrr', 'ndcg@5;10'])\n",
    "\n",
    "        AUC.append(result['group_auc'])\n",
    "        MRR.append(result['mean_mrr'])\n",
    "        NDCG5.append(result['ndcg@5'])\n",
    "        NDCG10.append(result['ndcg@510'])\n",
    "    \n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
