{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics\n",
    "#from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Description: Load tsv file\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "# Load tsv file\n",
    "News_vali = pd.read_csv('MINDsmall_dev/news.tsv', sep='\\t', header=None)\n",
    "News_vali.columns = ['news_id', 'category', 'subcategory', 'title', 'abstract', 'url', 'title_entities', 'abstract_entities']\n",
    "\n",
    "User_vali = pd.read_csv('MINDsmall_dev/behaviors.tsv', sep='\\t', header=None)\n",
    "User_vali.columns = ['impression_id', 'user_id', 'time', 'history', 'impressions']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data contains 17 topics and 264 subtopics\n"
     ]
    }
   ],
   "source": [
    "# Description: Load tsv file\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load tsv file\n",
    "News = pd.read_csv('MINDsmall_train/news.tsv', sep='\\t', header=None)\n",
    "News.columns = ['news_id', 'category', 'subcategory', 'title', 'abstract', 'url', 'title_entities', 'abstract_entities']\n",
    "News_vali = pd.read_csv('MINDsmall_dev/news.tsv', sep='\\t', header=None)\n",
    "News_vali.columns = ['news_id', 'category', 'subcategory', 'title', 'abstract', 'url', 'title_entities', 'abstract_entities']\n",
    "\n",
    "News_con = pd.concat([News, News_vali], ignore_index=True)\n",
    "\n",
    "\n",
    "UserData = pd.read_csv('MINDsmall_train/behaviors.tsv', sep='\\t', header=None)\n",
    "UserData.columns = ['impression_id', 'user_id', 'time', 'history', 'impressions']\n",
    "\n",
    "UserData = UserData.dropna()\n",
    "\n",
    "topic_size = News['category'].nunique()\n",
    "subtopic_size = News['subcategory'].nunique()\n",
    "\n",
    "print(f\"Data contains {topic_size} topics and {subtopic_size} subtopics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/magnusharder/Documents/UNI-DTU/6. Semester/Bachelor Projekt/News-Recommendations/.venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Define Vocabulary for users and topics\n",
    "from torchtext import vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import torch as th\n",
    "from LSTUR import GloVe\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "User_vocab = vocab.build_vocab_from_iterator([[id] for id in UserData['user_id']], specials=['<unk>'])\n",
    "User_vocab.set_default_index(User_vocab['<unk>'])\n",
    "News_vocab = vocab.build_vocab_from_iterator([[id] for id in  News_con['news_id']], specials=['<unk>'])\n",
    "News_vocab.set_default_index(News_vocab['<unk>'])\n",
    "Category_vocab = vocab.build_vocab_from_iterator([[Category] for Category in News['category']], specials=['<unk>'])\n",
    "Category_vocab.set_default_index(Category_vocab['<unk>'])\n",
    "Subcategory_vocab = vocab.build_vocab_from_iterator([[Category] for Category in News['subcategory']], specials=['<unk>'])\n",
    "Subcategory_vocab.set_default_index(Subcategory_vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Vocabulary for title and abstract\n",
    "max_title_length = max([len(tokenizer(title)) for title in News['title']])\n",
    "max_history_length = max([len(history.split(\" \")) for history in UserData['history']])\n",
    "max_history_length = 50 # Overwrite\n",
    "\n",
    "impressions_length = max([len(impressions.split(\" \")) for impressions in User_vali['impressions']])\n",
    "#max_impressions_length = 5 # Overwrite\n",
    "\n",
    "# Define Datapoint to tensor\n",
    "def Datapoint_to_Encodings(User):\n",
    "\n",
    "    History = News_vocab.lookup_indices(User.history.split(\" \"))\n",
    "    User_en = User_vocab.__getitem__(User.user_id)\n",
    "    Impressions = User.impressions.split(\" \")\n",
    "    Impressions,Clicked = map(list, zip(*[Impression.split(\"-\") for Impression in Impressions]))\n",
    "    \n",
    "    Positive, Negative = [],[]\n",
    "    for idx, click in enumerate(Clicked):\n",
    "        if click == \"1\":\n",
    "            Positive.append(Impressions[idx])\n",
    "        else:\n",
    "            Negative.append(Impressions[idx])\n",
    "\n",
    "    Impressions = [Positive[0]]\n",
    "\n",
    "   \n",
    "\n",
    "    if len(Negative) > 3:\n",
    "        for _ in random.sample(Negative,4):\n",
    "            Impressions.append(_)\n",
    "    else:\n",
    "        for _ in range(4):\n",
    "            Impressions.append(random.choice(Negative))\n",
    "\n",
    "    Clicked = [1,0,0,0,0]\n",
    "\n",
    "    # Shuffle\n",
    "    shuffled_index = [0,1,2,3,4]\n",
    "    random.shuffle(shuffled_index)\n",
    "\n",
    "\n",
    "    Impressions = [Impressions[i] for i in shuffled_index]\n",
    "    Clicked = [Clicked[i] for i in shuffled_index]\n",
    "\n",
    "\n",
    "    # Convert to tensor\n",
    "    Impressions = News_vocab.lookup_indices(Impressions)\n",
    "    History, User_en, Impressions, Clicked = map(th.tensor, [History, User_en, Impressions, Clicked])\n",
    "\n",
    "    return History, User_en, Impressions, Clicked\n",
    "\n",
    "# Define Datapoint to tensor\n",
    "def Datapoint_to_Encodings_vali(User):\n",
    "\n",
    "    History = News_vocab.lookup_indices(User.history.split(\" \"))\n",
    "    User_en = User_vocab.__getitem__(User.user_id)\n",
    "    Impressions = User.impressions.split(\" \")\n",
    "    Impressions,Clicked = map(list, zip(*[Impression.split(\"-\") for Impression in Impressions]))\n",
    "    \n",
    "\n",
    "    # Convert to tensor\n",
    "    Impressions = News_vocab.lookup_indices(Impressions)\n",
    "    Clicked = [int(click) for click in Clicked]\n",
    "\n",
    "    History, User_en, Impressions, Clicked = map(th.tensor, [History, User_en, Impressions, Clicked])\n",
    "\n",
    "    return History, User_en, Impressions, Clicked\n",
    "\n",
    "# Pack Title\n",
    "def pack_Title(title,max_length):\n",
    "\n",
    "    src_len, _ = title.size()\n",
    "\n",
    "    title_reformated = th.zeros(max_length,300)\n",
    "\n",
    "    title_reformated[:src_len,:] = title\n",
    "\n",
    "    return title_reformated, src_len\n",
    "\n",
    "\n",
    "# Get Numeric Artikles representation\n",
    "def get_Article_Encodings(Artikle):\n",
    "\n",
    "\n",
    "    title = GloVe.get_vecs_by_tokens(tokenizer(Artikle['title']))\n",
    "    \n",
    "    #Abstract = [tokenizer(abstract) for abstract in Artikle['abstract']]\n",
    "    Category = Category_vocab.__getitem__(Artikle['category'])\n",
    "    Subcategory = Subcategory_vocab.__getitem__(Artikle['subcategory'])\n",
    "\n",
    "    title, title_len = pack_Title(title,max_title_length)\n",
    "\n",
    "    Category, Subcategory, title_len = map(th.tensor, [Category, Subcategory, title_len])\n",
    "\n",
    "    \n",
    "\n",
    "    return Category, Subcategory, title, title_len\n",
    "\n",
    "# Store all News in Dictionary for faster access\n",
    "News_tensors = {}\n",
    "\n",
    "for i in range(len(News_con)):\n",
    "    News_tensors[News_vocab.__getitem__(News_con['news_id'][i])] = get_Article_Encodings(News_con.loc[i])\n",
    "\n",
    "# Get Numeric User representation\n",
    "def Datapoint_to_tensor(User,train=True):\n",
    "\n",
    "    if train:\n",
    "        History, User_en, Impressions, Clicked = Datapoint_to_Encodings(User)\n",
    "        max_impressions_length = 5\n",
    "        if random.random() < 0.4:\n",
    "            User_en = 0 # Mask user \n",
    "    else:\n",
    "        max_impressions_length = impressions_length\n",
    "        History, User_en, Impressions, Clicked = Datapoint_to_Encodings_vali(User)\n",
    "\n",
    "\n",
    "\n",
    "    History_tensor = th.zeros(max_history_length,max_title_length,300)\n",
    "    Category = th.zeros(max_history_length)\n",
    "    Subcategory = th.zeros(max_history_length)\n",
    "    history_len = min(len(History),max_history_length)\n",
    "\n",
    "    for idx,article in enumerate(History[-history_len:]):\n",
    "        Category[idx], Subcategory[idx], History_tensor[idx], _ = News_tensors[article.item()]\n",
    "\n",
    "    Impressions_tensor = th.zeros(max_impressions_length,max_title_length,300)\n",
    "    Category_Impressions = th.zeros(max_impressions_length)\n",
    "    Subcategory_Impressions = th.zeros(max_impressions_length)\n",
    "    Impressions_len = len(Impressions)\n",
    "\n",
    "    history_len, Impressions_len = map(th.tensor, [history_len, Impressions_len])\n",
    "\n",
    "\n",
    "    for idx,article in enumerate(Impressions):\n",
    "        Category_Impressions[idx], Subcategory_Impressions[idx], Impressions_tensor[idx], _ = News_tensors[article.item()]\n",
    "    \n",
    "    Clicked = Clicked.argmax()\n",
    "\n",
    "    return User_en, Category, Subcategory, History_tensor, history_len, Category_Impressions, Subcategory_Impressions, Impressions_tensor, Impressions_len, Clicked\n",
    "\n",
    "\n",
    "# Def load batch\n",
    "def load_batch(User, batch_size, device='cpu',train=True):\n",
    "    \n",
    "        #User = User.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "        for i in range(0, len(User), batch_size):\n",
    "    \n",
    "            User_batch = User[i:i+batch_size]\n",
    "    \n",
    "            User_en = []\n",
    "            Category = []\n",
    "            Subcategory = []\n",
    "            History_tensor = []\n",
    "            history_len = []\n",
    "            Category_Impressions = []\n",
    "            Subcategory_Impressions = []\n",
    "            Impressions_tensor = []\n",
    "            Impressions_len = []\n",
    "            Clicked = []\n",
    "    \n",
    "            for i in range(len(User_batch)):\n",
    "                User_en_, Category_, Subcategory_, History_tensor_, history_len_, Category_Impressions_, Subcategory_Impressions_, Impressions_tensor_, Impressions_len_, Clicked_ = Datapoint_to_tensor(User_batch.iloc[i],train=train)\n",
    "                User_en.append(User_en_)\n",
    "                Category.append(Category_)\n",
    "                Subcategory.append(Subcategory_)\n",
    "                History_tensor.append(History_tensor_)\n",
    "                history_len.append(history_len_)\n",
    "                Category_Impressions.append(Category_Impressions_)\n",
    "                Subcategory_Impressions.append(Subcategory_Impressions_)\n",
    "                Impressions_tensor.append(Impressions_tensor_)\n",
    "                Impressions_len.append(Impressions_len_)\n",
    "                Clicked.append(Clicked_)\n",
    "    \n",
    "            User_en, Category, Subcategory, History_tensor, history_len, Category_Impressions, Subcategory_Impressions, Impressions_tensor, Impressions_len, Clicked = map(th.stack, [User_en, Category, Subcategory, History_tensor, history_len, Category_Impressions, Subcategory_Impressions, Impressions_tensor, Impressions_len, Clicked])\n",
    "            User_en, Category, Subcategory, history_len, Category_Impressions, Subcategory_Impressions, Impressions_len, Clicked = map(lambda x: x.long(), [User_en, Category, Subcategory, history_len, Category_Impressions, Subcategory_Impressions, Impressions_len, Clicked])\n",
    "            yield User_en.to(device), Category.to(device), Subcategory.to(device), History_tensor.to(device), history_len.to(device), Category_Impressions.to(device), Subcategory_Impressions.to(device), Impressions_tensor.to(device), Impressions_len.to(device), Clicked.to(device)\n",
    "\n",
    "            #yield User_en, Category, Subcategory, History_tensor, history_len, Category_Impressions, Subcategory_Impressions, Impressions_tensor, Impressions_len, Clicked\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "from LSTUR import LSTUR_con\n",
    "from torch import nn,optim\n",
    "device = \"cpu\"\n",
    "\n",
    "LSTUR_con_module = LSTUR_con(\n",
    "    seq_len = max_history_length,\n",
    "    user_dim=300,\n",
    "    user_size=User_vocab.__len__(),\n",
    "    topic_size=Category_vocab.__len__(),\n",
    "    topic_dim=100,\n",
    "    subtopic_size=Subcategory_vocab.__len__(),\n",
    "    subtopic_dim=100,\n",
    "    word_dim=300,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "Softmax = nn.Softmax(dim=1)\n",
    "Softmax2 = nn.Softmax(dim=0)\n",
    "labels = np.arange(0,295)\n",
    "\n",
    "BatchLoader = load_batch(UserData, batch_size=10,train=False,device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43452380952380953\n",
      "0.43452380952380953\n",
      "0.43452380952380953\n",
      "0.43452380952380953\n",
      "0.43452380952380953\n",
      "0.43452380952380953\n",
      "0.43452380952380953\n",
      "0.43452380952380953\n",
      "0.43452380952380953\n",
      "0.43452380952380953\n"
     ]
    }
   ],
   "source": [
    "User_en, Category, Subcategory, History_tensor, history_len, Category_Impressions, Subcategory_Impressions, Impressions_tensor, Impressions_len, Clicked = BatchLoader.__next__()\n",
    "\n",
    "y_true = th.empty(0)\n",
    "preds = th.empty((0,295))\n",
    "for i in range(10):\n",
    "    output = LSTUR_con_module(User_en, Category, Subcategory, History_tensor, history_len, Category_Impressions, Subcategory_Impressions, Impressions_tensor)\n",
    "\n",
    "    output = Softmax(output)\n",
    "    preds = th.cat((preds,output),0)\n",
    "    y_true = th.cat((y_true,Clicked),0)\n",
    "\n",
    "    print(roc_auc_score(y_true.detach(),preds.detach(),multi_class=\"ovo\",labels=np.arange(0,295)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 295])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6., 17.,  1.,  6., 16.,  1.,  7.,  2., 18.,  7.,  6., 17.,  1.,  6.,\n",
       "        16.,  1.,  7.,  2., 18.,  7.,  6., 17.,  1.,  6., 16.,  1.,  7.,  2.,\n",
       "        18.,  7.,  6., 17.,  1.,  6., 16.,  1.,  7.,  2., 18.,  7.,  6., 17.,\n",
       "         1.,  6., 16.,  1.,  7.,  2., 18.,  7.,  6., 17.,  1.,  6., 16.,  1.,\n",
       "         7.,  2., 18.,  7.,  6., 17.,  1.,  6., 16.,  1.,  7.,  2., 18.,  7.,\n",
       "         6., 17.,  1.,  6., 16.,  1.,  7.,  2., 18.,  7.,  6., 17.,  1.,  6.,\n",
       "        16.,  1.,  7.,  2., 18.,  7.,  6., 17.,  1.,  6., 16.,  1.,  7.,  2.,\n",
       "        18.,  7.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 18,  1,  7, 24,  4,  6,  5, 30, 15,  5, 18,  1, 10, 24,  4,  6,  5,\n",
       "        21, 15,  5, 18,  1, 10, 24,  4,  6,  5, 21, 15,  5, 18,  1, 10, 24,  4,\n",
       "         6,  5, 21, 15,  5, 18,  1, 10, 24,  4,  6,  5, 21, 15,  5, 18,  1,  7,\n",
       "        24,  4,  6,  5, 21, 15,  5, 18,  1, 10, 24,  4,  6,  5, 21, 15,  5, 18,\n",
       "         1, 10, 24,  4,  6,  5, 21, 15,  5, 18,  1, 10, 24,  4,  6,  5, 21, 15,\n",
       "         5, 18,  1, 10, 24,  4,  6,  5, 21, 15])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Singleton array array(93) cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m probs \u001b[39m=\u001b[39m output[i][:Impressions_len[i]]\n\u001b[1;32m      5\u001b[0m probs \u001b[39m=\u001b[39m Softmax2(probs)\n\u001b[0;32m----> 7\u001b[0m score \u001b[39m=\u001b[39m roc_auc_score(Clicked[i],probs\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\u001b[39m1\u001b[39;49m),multi_class\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39movo\u001b[39;49m\u001b[39m'\u001b[39;49m,labels\u001b[39m=\u001b[39;49mlabels)    \n",
      "File \u001b[0;32m~/Documents/UNI-DTU/6. Semester/Bachelor Projekt/News-Recommendations/.venv/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:550\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) \\\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[39mfrom prediction scores.\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[39marray([0.81..., 0.84... , 0.93..., 0.87..., 0.94...])\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    549\u001b[0m y_type \u001b[39m=\u001b[39m type_of_target(y_true, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 550\u001b[0m y_true \u001b[39m=\u001b[39m check_array(y_true, ensure_2d\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, dtype\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    551\u001b[0m y_score \u001b[39m=\u001b[39m check_array(y_score, ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    553\u001b[0m \u001b[39mif\u001b[39;00m y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m (\n\u001b[1;32m    554\u001b[0m     y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m y_score\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m y_score\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m    555\u001b[0m ):\n\u001b[1;32m    556\u001b[0m     \u001b[39m# do not support partial ROC computation for multiclass\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/UNI-DTU/6. Semester/Bachelor Projekt/News-Recommendations/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:929\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    921\u001b[0m         _assert_all_finite(\n\u001b[1;32m    922\u001b[0m             array,\n\u001b[1;32m    923\u001b[0m             input_name\u001b[39m=\u001b[39minput_name,\n\u001b[1;32m    924\u001b[0m             estimator_name\u001b[39m=\u001b[39mestimator_name,\n\u001b[1;32m    925\u001b[0m             allow_nan\u001b[39m=\u001b[39mforce_all_finite \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mallow-nan\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    926\u001b[0m         )\n\u001b[1;32m    928\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 929\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n\u001b[1;32m    930\u001b[0m     \u001b[39mif\u001b[39;00m n_samples \u001b[39m<\u001b[39m ensure_min_samples:\n\u001b[1;32m    931\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    932\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m sample(s) (shape=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) while a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    933\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m minimum of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m is required\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    934\u001b[0m             \u001b[39m%\u001b[39m (n_samples, array\u001b[39m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m    935\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/UNI-DTU/6. Semester/Bachelor Projekt/News-Recommendations/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:335\u001b[0m, in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m x\u001b[39m.\u001b[39mshape \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(x\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 335\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    336\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mSingleton array \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m cannot be considered a valid collection.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m x\n\u001b[1;32m    337\u001b[0m         )\n\u001b[1;32m    338\u001b[0m     \u001b[39m# Check that shape is returning an integer or default to len\u001b[39;00m\n\u001b[1;32m    339\u001b[0m     \u001b[39m# Dask dataframes may not return numeric shape[0] value\u001b[39;00m\n\u001b[1;32m    340\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], numbers\u001b[39m.\u001b[39mIntegral):\n",
      "\u001b[0;31mTypeError\u001b[0m: Singleton array array(93) cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    labels = np.arange(0,Impressions_len[i])\n",
    "\n",
    "    probs = output[i][:Impressions_len[i]]\n",
    "    probs = Softmax2(probs)\n",
    "\n",
    "    score = roc_auc_score(Clicked[i],probs.reshape(-1,1),multi_class='ovo',labels=labels)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.4204e-01, 1.0898e-11, 1.1135e-11, 1.1766e-12, 1.0373e-10, 1.0764e-11,\n",
       "        1.4769e-08, 6.7888e-15, 3.7516e-10, 8.6651e-21, 1.3959e-02, 1.0878e-11,\n",
       "        2.0267e-12, 1.7289e-06, 1.0610e-11, 2.7515e-02, 1.6191e-06, 2.4694e-03,\n",
       "        2.6912e-17, 1.4003e-02, 1.0098e-10, 3.7744e-10, 3.4400e-08, 3.4051e-08,\n",
       "        3.4408e-08, 3.4332e-08, 3.4264e-08, 3.4118e-08, 3.4378e-08, 3.4197e-08,\n",
       "        3.4400e-08, 3.4174e-08, 3.4278e-08, 3.4335e-08, 3.4385e-08, 3.4212e-08,\n",
       "        3.4237e-08, 3.4116e-08, 3.4138e-08, 3.4199e-08, 3.4400e-08, 3.3857e-08,\n",
       "        3.4236e-08, 3.4153e-08, 3.4257e-08, 3.4162e-08, 3.4271e-08, 3.4096e-08,\n",
       "        3.4128e-08, 3.4290e-08, 3.3993e-08, 3.4111e-08, 3.4159e-08, 3.4301e-08,\n",
       "        3.4265e-08, 3.4082e-08, 3.4025e-08, 3.4189e-08, 3.4333e-08, 3.4247e-08,\n",
       "        3.4100e-08, 3.4199e-08, 3.4150e-08, 3.4372e-08, 3.4351e-08, 3.4265e-08,\n",
       "        3.4282e-08, 3.4147e-08, 3.4199e-08, 3.4037e-08, 3.4121e-08, 3.4073e-08,\n",
       "        3.4046e-08, 3.4313e-08, 3.4202e-08, 3.4001e-08, 3.4019e-08, 3.4313e-08,\n",
       "        3.4205e-08, 3.4178e-08, 3.4205e-08, 3.4208e-08, 3.4046e-08, 3.4315e-08,\n",
       "        3.4271e-08, 3.4159e-08, 3.4099e-08, 3.4384e-08, 3.4016e-08, 3.4187e-08,\n",
       "        3.4201e-08, 3.4211e-08, 3.4177e-08, 3.4083e-08, 3.4393e-08, 3.4325e-08,\n",
       "        3.4290e-08, 3.3947e-08, 3.4234e-08, 3.4212e-08, 3.4138e-08, 3.4236e-08,\n",
       "        3.3906e-08, 3.4186e-08, 3.4186e-08, 3.4036e-08, 3.4244e-08, 3.4206e-08,\n",
       "        3.4061e-08, 3.4346e-08, 3.4275e-08, 3.4256e-08, 3.4064e-08, 3.4257e-08,\n",
       "        3.4157e-08, 3.4049e-08, 3.4069e-08, 3.3955e-08, 3.4009e-08, 3.4148e-08,\n",
       "        3.3996e-08, 3.4111e-08, 3.4220e-08, 3.4080e-08, 3.4418e-08, 3.4099e-08,\n",
       "        3.4209e-08, 3.4022e-08, 3.4106e-08, 3.4071e-08, 3.4327e-08, 3.4148e-08,\n",
       "        3.4128e-08, 3.4128e-08, 3.4154e-08, 3.4235e-08, 3.4338e-08, 3.4255e-08,\n",
       "        3.4185e-08, 3.4128e-08, 3.4296e-08, 3.4232e-08, 3.4115e-08, 3.4228e-08,\n",
       "        3.4171e-08, 3.4280e-08, 3.4028e-08, 3.4147e-08, 3.4158e-08, 3.4157e-08,\n",
       "        3.4180e-08, 3.3935e-08, 3.4252e-08, 3.4100e-08, 3.4336e-08, 3.3970e-08,\n",
       "        3.4257e-08, 3.4229e-08, 3.4097e-08, 3.4143e-08, 3.4219e-08, 3.4101e-08,\n",
       "        3.4059e-08, 3.4181e-08, 3.4052e-08, 3.4315e-08, 3.4300e-08, 3.4102e-08,\n",
       "        3.4183e-08, 3.4144e-08, 3.4091e-08, 3.4243e-08, 3.4188e-08, 3.4148e-08,\n",
       "        3.4253e-08, 3.4184e-08, 3.4204e-08, 3.4104e-08, 3.4384e-08, 3.4210e-08,\n",
       "        3.4287e-08, 3.4235e-08, 3.4203e-08, 3.4095e-08, 3.4276e-08, 3.4104e-08,\n",
       "        3.4428e-08, 3.4223e-08, 3.4204e-08, 3.4114e-08, 3.4027e-08, 3.4186e-08,\n",
       "        3.4373e-08, 3.4246e-08, 3.4241e-08, 3.4073e-08, 3.4123e-08, 3.4227e-08,\n",
       "        3.4260e-08, 3.4241e-08, 3.4084e-08, 3.4159e-08, 3.4071e-08, 3.4435e-08,\n",
       "        3.4175e-08, 3.4154e-08, 3.3998e-08, 3.4346e-08, 3.4088e-08, 3.4426e-08,\n",
       "        3.4322e-08, 3.4212e-08, 3.4162e-08, 3.4109e-08, 3.4292e-08, 3.4210e-08,\n",
       "        3.4230e-08, 3.4250e-08, 3.4182e-08, 3.4427e-08, 3.4285e-08, 3.4127e-08,\n",
       "        3.3991e-08, 3.4279e-08, 3.4110e-08, 3.4089e-08, 3.4116e-08, 3.4349e-08,\n",
       "        3.4236e-08, 3.4096e-08, 3.4098e-08, 3.4365e-08, 3.4170e-08, 3.4180e-08,\n",
       "        3.4262e-08, 3.4153e-08, 3.3982e-08, 3.4130e-08, 3.4198e-08, 3.4276e-08,\n",
       "        3.4139e-08, 3.4229e-08, 3.4204e-08, 3.4136e-08, 3.4340e-08, 3.4074e-08,\n",
       "        3.4129e-08, 3.4284e-08, 3.4225e-08, 3.4081e-08, 3.4124e-08, 3.4028e-08,\n",
       "        3.4129e-08, 3.4317e-08, 3.4220e-08, 3.4235e-08, 3.4078e-08, 3.4249e-08,\n",
       "        3.4226e-08, 3.4260e-08, 3.4180e-08, 3.4203e-08, 3.4343e-08, 3.4177e-08,\n",
       "        3.4250e-08, 3.4307e-08, 3.4328e-08, 3.4050e-08, 3.4086e-08, 3.4097e-08,\n",
       "        3.4163e-08, 3.4189e-08, 3.4231e-08, 3.4327e-08, 3.4239e-08, 3.4159e-08,\n",
       "        3.4245e-08, 3.4187e-08, 3.4225e-08, 3.4043e-08, 3.4130e-08, 3.4308e-08,\n",
       "        3.4351e-08, 3.4105e-08, 3.4042e-08, 3.4194e-08, 3.4453e-08, 3.4168e-08,\n",
       "        3.4252e-08, 3.4036e-08, 3.4232e-08, 3.4233e-08, 3.4250e-08, 3.4194e-08,\n",
       "        3.4215e-08])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Softmax(output.cpu().detach())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.611111111111111"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "take= np.array([0,2,0])\n",
    "\n",
    "#take = [(0,1),(1,2),(2,2)]\n",
    "g = np.array([[1,3,2],[4,5,6],[8,11,7]])\n",
    "\n",
    "\n",
    "order = np.argsort(g,axis=1).argsort(axis=1)\n",
    "order_test = np.argsort(g,axis=1)[::-1]\n",
    "#print(order)\n",
    "\n",
    "ranks = np.array([np.take(order[i],take[i]) for i in range(3)])\n",
    "\n",
    "(1/(3-ranks)).mean()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2161e111f783a6322a6ae262a47844d9386d7dfb61a436620c434d93864cb0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
